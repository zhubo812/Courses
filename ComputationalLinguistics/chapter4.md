## 第四章 语言模型
~~~~
### 主要内容
1. 基本概念
2. 参数估计
3. 数据平滑
4. 语言模型的自适应
5. 神经概率语言模型
6. 语言模型的评价方法
7. 语言模型的应用

~~~~
### 4.1 基本概念
- 大规模语料库的出现为自然语言统计处理方法的实现提供了可能，统计方法的成功使用推动了语料库语言学的发展。

- 基于大规模语料库和统计方法可以用于发现语言使用的普遍规律，通过机器学习自动获取语言知识推测未知语言现象。

- 如何计算一个句子出现的概率？
~~~~
例如：
“人不能两次踏入同一条河流，因为无论是这条河还是这个人都已经不同。”

- 以句子为单位计算其在所有使用过的句子中的概率？

- 根据句子构成单位计算其联合概率？

~~~~
#### 4.1.1 什么是语言模型

$$
P(s)=P(w_1)\times P(w_2|w_1)\times P(w_3|w_1w_2)\times ...
$$
$$
\times P(w_n|w_1...w_{n-1})
=\prod P(w_i|w_1...w_{i-1})
$$
当`$i=1$`时,`$P(w_1|w_0)=P(w_1)$`,这就是语言模型。
~~~~
需要注意的是
- `$w_i$`可以是字、词、短语或词类等等，称为统计基元。通常以“词”代称。  
- `$w_i$`的概率由 `$w_0...w_{i-1}$`决定,由特定的一组`$w_0...w_{i-1}$`构成的一个序列，称为`$w_i$`的历史。
~~~~
问题：

- 随着历史基元数量的增加，不同的“历史”(路径)成指数增长。对于第`$i(i>1)$`个统计基元,历史基元的数量为`$i-1$`个。如果共有`$L$`个不同的基元,如词表。理论上每一个词语都有可能出现在1到`$i-1$`的位置上，第`$i$`个基元就有`$L_{i-1}$`中不同的历史。这样，在考虑所有`$L_{i-1}$`种不同历史的情况下产生的第`$i$`个基元的概率，模型中就会有`$𝐿_𝑚$`个自由参数`$P(w_n|w_1...w_{n-1})$`  
- 如果`$L=5000,m=3$`,自由参数的数目为1250亿！

~~~~
解决办法：

- 减少历史基元的个数,将`$w_1w_2...w_{i-1}$`映射到等价类 `$S(w_1w_2...w_{i-1})$`使等价类的数目远远小于原来不同历史基元的数目。

- 则有：  
`$P(w_i|w_1...w_{i-1})=P(w_i|S(w_1...w_{i-1}))$`


这就是划分等价类的方法

~~~~
#### 4.1.2 N元语言模型

这种情况下的语言模型称为n元文法(n-gram)。
1. 当`$n=1$`时,即出现在第`$i$`位上的基元`$w_i$`独立于历史,n-gram 被称为一阶马尔柯夫链(uni-gram 或monogram)
2. 当`$n=2$`时,n-gram被称为2阶马尔柯夫链(bi-gram)
3. 当`$n=3$`时,n-gram被称为3阶马尔柯夫链(tri-gram)

~~~~

为了保证条件概率在`$i=1$`时有意义,同时为了保证句子内所有字符串的概率和为 1,即`$\Sigma_sp(s)=1$`,可以在句子首尾两端增加两个标志:
<BOS>`$w_1w_2...w_m$`<EOS>不失一般性，对于`$n>2$`的n-gram`$P(s)$`可以分解为：
$$
P(s)= \prod_{i=1}^{m+1}P(w_i|w_{i-n+1}^{i-1})
$$
其中,`$w_i^j$`表示词序列`$w_i...w_j$`,约定`$w_0$`为\<BOS\>,`$w_{m+1}$`为\<EOS\>。
~~~~
例如，给定句子："John read a book"
- 首先，增加标记：\<BOS\> John read a book \<EOS\>

- 然后，2元文法的概率为：
`$P(John\ read\ a\ book) = $`
`$P( John|<BOS> )\times P(read | John)$`
`$\times P(a|read)\times P(book|a)\times P(<EOS> | book )$`
~~~~

例如，给定拼音串：ta shi yan jiu sheng wu de

可能的汉字串:  
1. 踏实研究生物的  
2. 他实验救生物的  
3. 他使烟酒生物的  
4. 他是研究生雾的  
5. … …
~~~~

如果使用2-gram：

`$P(CString1) =P(踏实|<BOS>)\times P(研究|踏实)\times$`
`$P(生物|研究)\times P(的|生物)\times P(<EOS>|的)$`

`$P(CString2) =P(他|<BOS>)\times P(实验|他)\times P(救|实验)$`
`$\times P(生物|救)\times P(的|生物)\times P(<EOS>|的)$`

`$P(CString3) =P(他|<BOS>)\times P(是|他)\times P(研究|是)$`
`$\times P(生物|研究)\times P(的|生物)\times P(<EOS>|的)$`
……
~~~~
#### N元语法模型生成字符串的规模
当语料的词总数为`$N$`时
- 一元语法：
	1. 样本空间为`$N$`
	2. 只选择使用频率最高的汉字
- 二元语法：
	1. 样本空间为`$N^2$`
	2. 效果比一元语法明显提高

~~~~
微软拼音输入法基于n-gram  
对汉字而言四元语法效果会好一些
~~~~
例如,汉语分词问题  

给定汉字串：他是研究生物的。  

- 可能切分出的字符串组合：  
	1. 他|是|研究生|物|的  
	2. 他|是|研究|生物|的  
~~~~
如果使用2-gram：  

`$P(Seg1)=P(他|<BOS>)\times P(是|他)\times P(研究生|是)$`
`$\times P(物|研究生)\times P(的|物|)\times P(<EOS>|的) $` 

`$P(Seg2)=P(他|<BOS>)\times P(是|他)\times P(研究|是)$`
`$\times P(生物|研究)\times P(的|生物|)\times P(<EOS>|的)$`
~~~~
#### 问题：如何获得n元语法模型？

~~~~
#### 4.1.3 两个重要概念：
- 训练语料(training data)：用于建立模型，确定模型参数的已知语料。
- 最大似然估计(maximum likelihood Evaluation,MLE)：用相对频率计算概率的方法。
参数：用来计算`$p(w|h)$` 的数值,从训练语料数据中得到
~~~~

#### 数据准备
- 去掉格式符号
- 定义词的边界
- 定义句子边界(插入\<BOS\>和\<EOS\>等记号)
- 字母的大小写（保留、忽略或者智能识别）
- 数字（保留、替换为\<num\>等）
- ……
~~~~
### 4.2 参数估计
#### 4.2.1 n-gram参数估计
对于n-gram,参`$P(w_i|w_{i-n+1}^{i-1})$`数可由最大似然估计求得
$$
P(w_i|w_{i-n+1}^{i-1})=f(w_i|w_{i-n+1}^{i-1})=\frac{c(w_{i-n+1}^i)}{\sum_{w_i}c(w_{i-n+1}^i)}
$$
~~~~
其中,`$\sum_{w_i}c(w_{i-n+1}^i)$`是历史字符串`$w_{i-n+1}^{i-1}$`在给定语料中出现的次数,即`$c(w_{i-n+1}^{i-1})$`,不论`$w_i$`是什么。  
`$P(w_i|w_{i-n+1}^{i-1})$`是在给定`$w_{i-n+1}^{i-1}$`的条件下`$w_i$`出现的相对频率,分子为`$w_{i-n+1}^{i-1}$`与`$w_i$`同现的次数。

~~~~
例如，给定训练语料：  
- “John read Moby Dick”  
- “Mary read a different book”  
- “She read a book by Carl”  
~~~~
根据二元文法求句子的概率,把语料处理成如下：  

- \<BOS\> John read Moby Dick\<EOS\>  
- \<BOS\> Mary read a different book\<EOS\>  
- \<BOS\> She read a book by Carl\<EOS\>  
~~~~
- `$P(John|<BOS>)=\frac{c(<BOS>\ John)}{\sum_wc(<BOS>\ w)}=\frac{1}{3}$`  
- `$P(read|John)=\frac{c(John\ read)}{\sum_wc(John\ w)}=\frac{1}{1}$`  
- `$P(a|read)=\frac{c(read\ a)}{\sum_wc(read\ w)}=\frac{2}{3}$`  
- `$P(book|a)=\frac{c(a\ book)}{\sum_wc(a\ w)}=\frac{1}{2}$`  
- `$P(<EOS>|book)=\frac{c(book\ <EOS>)}{\sum_wc(book\ w)}=\frac{1}{2}$`
- `$P(John\ read\ a\ book)=\frac{1}{3}\times 1\times \frac{2}{3}\times \frac{1}{2}\times \frac{1}{2}\approx 0.06$`

~~~~
- `$P(Carl\ read\ a\ book)=?$`  
`$P(Carl\ read\ a\ book)=P(Carl|<BOS>)\times $`
`$P(read|Carl)\times P(a|read)\times$`
`$ P(book|a)\times P(<EOS>|book)$`  
- `$P(Carl|<BOS>)=\frac{c(<BOS>\ Carl)}{\sum_wc(<BOS>\ w)}=\frac{0}{3}=0$`  
- `$P(read|Carl)=\frac{c(Carl\ read)}{\sum_wc(Carl\ w)}=\frac{0}{1}=0$`  
- `$P(Carl\ read\ a\ book)=0$` 
~~~~
<!--
  

-->
#### 4.2.3 问题:数据稀疏(Data Sparsness)

~~~~
- 给定训练样本中未观察到的事件赋以0概率。
- 若某n-gram在训练语料中没有出现,则该n-gram的概率必定是0。
~~~~
- 最直接的解决的办法是扩大训练语料的规模。但是无论怎样扩大训练语料，都不可能保证所有的词在训练语料中均出现。
~~~~
- 由于训练样本不足而导致所估计的分布不可靠的问题，称为数据稀疏问题。
- 在NLP领域中，数据稀疏问题永远存在，不太可能有一个足够大的训练语料，因为语言中的大部分词都属于低频词。

~~~~

#### 4.2.4 解决数据稀疏问题
对语言而言，由于数据稀疏的存在，最大似然估计(MLE) 不是一种很好的参数估计办法。 
~~~~
#### 最大似然估计

例S = \<BOS\> He can buy you the can of soda  \<EOS\>   
- Unigram: (8 words in vocabulary)  
	- `$p_1(He) = p_1(buy) = p_1 (you) = p_1 (the) = p_1(of) = p_1(soda)= 0.125  $`  
	- `$ p_1(can) = 0.25$`
~~~~

- Bigram:  
	- `$p_2(He|<BOS>)=p_2(can|He) = 1$`  
	- `$p_2(buy|can)=p_2(of|can) = 0.5$`
	- `$,p_2(you|buy)= 1,...$`

~~~~
1. 解决办法: 平滑技术
- 把在训练样本中出现过的事件的概率适当减小。即从大于0的MLE概率估算`$p(w)$`中得到`$p'(w)$`,`$p'(w)<p(w)$`
- 把减小得到的概率密度分配给训练语料中没有出现过的事件。即
`$\sum(p(w)-p'(w))=D$`
把`$D$`分配到`$p(w)=0$`的事件中，保证`$\sum p'(w)=1$`,这个过程有时也称为discounting(减值)
~~~~
2. 基本目标：测试样本的语言模型困惑度越小越好。
3. 基本约束：`$\sum_{w_i}P(w_i|w_1,w_2,...,w_{i-1})=1$`
~~~~
- 困惑度是交叉熵的指数形式
- 交叉熵
$$
H_p(T)=\frac{1}{W_T}\log P(T)
$$
其中，`$W_T$`是测试文本的词数。

- 困惑度
$$Perplexity=2^{H(T)}$$
~~~~
- 例S = \<BOS\> He can buy you the can of soda  \<EOS\>   
- 例 S = \<BOS\> It was the greatest buy of all \<EOS\>  

`$H_S(p_1)=H_S(p_2)=H_S(p_3) =\infty$`  
原因是：  
1. 所有的unigram除了`$p_1(the)$`, `$p_1(buy)$`, `$p_1(of)$`都是0
2. 所有bigram的概率都是 0.  
3. 所有trigram的概率都是 0.  
~~~~
我们希望使每个概率都是非零的

~~~~
### 4.3 数据平滑
目前已经提出了很多数据平滑技术，如：
- Add-one 平滑
- Add-delta 平滑
- Witten-Bell平滑
- Good-Turing平滑
- Church-Gale平滑
- Jelinek-Mercer平滑
- Katz平滑
- ……
~~~~

#### 4.3.1 Add-one平滑(Laplace’s law)
基本思想: 每一种情况出现的次数加1. 
`$count_{new}(n-gram)=count_{old}(n-gram) + 1$`  
没有出现过的n-gram的概率不再是0
~~~~

平滑后的概率计算公式为：
$$
P_{addone}(w_1w_2..w_n)=\frac{c(w_1w_2...w_n)+1}{N+V}
$$

其中,`$N$`为训练语料中所有的n-gram的数量(token)  
`$V$`为所有的可能的不同的n-gram的数量(type)  

~~~~
全部概率之和为：  
$$
\sum P_{addone}(w_1w_2...w_n)=\sum \frac{c(w_1w_2...w_n)+1}{N+V}
$$
$$
=\frac {\sum {c(w_1w_2...w_n)+1}}{N+V}
=\frac {N+V}{N+V}=1
$$
其中，`$T$`为训练数据,`$V$`为词表,`$w$`为词

- Add-one平滑方法是出现n次的词，假设出现n+1次，这种方法最简单但效果较差。
~~~~


#### 4.3.2 Add-one平滑的问题
1. 训练语料中未出现的n-gram的概率不再为0,而是一个大于0的较小的概率值。
2. 但由于训练语料中未出现n-gram数量太多,平滑后,所有未出现的n-gram占据了整个概率分布中的一个很大的比例。
~~~~
3. 在NLP中,Add-one给训练语料中没有出现过的n-gram分配了太多的概率空间。
4. 认为所有未出现的n-gram概率相等,这是否合理？
5. 出现在训练语料中的那些n-gram,都增加同样的频度值,这是否公平？(低频、高频)
~~~~

各种平滑方法的详细介绍和比较请参阅

[Stanley Chen的相关研究](http://www.cs.cmu.edu/~sfc/html/publications.html)

~~~~


### 4.3 语言模型的自适应
- 一方面,在训练语言模型时所采用的语料往往来自多种不同的领域,这些综合性语料难以反映不同领域之间在语言使用规律上的差异,而语言模型恰恰对于训练文本的类型、主题和风格等都十分敏感；
- 另一方面,`$n$`元语言模型的独立性假设的前提是一个文本中的当前词出现的概率只与它前面相邻的`$n-1$`个词相关,但这种假设在很多情况下是明显不成立的。
~~~~
#### 4.3.1  自适应的问题
1. 跨领域的脆弱性
2. 独立性假设的无效性
3. 提高语言模型对语料的领域、主题、类型等因素的适应性
~~~~
#### 4.3.2 自适应方法
1. 基于缓存的语言模型(cache-based LM)
2. 基于混合方法的语言模型
3. 基于最大熵的语言模型
~~~~
##### (1) 基于缓存的语言模型(cache-based LM)
该方法针对的问题是：在文本中刚刚出现过的一些词在后边的句子中再次出现的可能性往往较大，比标准的n-gram模型预测的概率要大。针对这种现象，cache-based自适应方法的基本思路是：语言模型通过n-gram的线性插值求得：

$$
\hat P(w_i|w_1^{i-1})=\lambda\hat P_{Cache}(w_i|w_1^{i-1})
$$
$$
+(1-\lambda)\hat P_{n-gram}(w_i|w_{i-n+1}^{i-1})
$$
~~~~
通常的处理方法是：在缓存中保留前面的`$K$`个单词，每个词的概率（缓存概率）用其在缓存中出现的相对频率计算得出：
$$
\hat P_{Cache}(w_i|w_1^{i-1})=\frac{1}{k}\sum_{j=i-K}^{i-1}I_{w_j=w_i}
$$
其中,`$I_{\epsilon}"$`为指示器函数(indicator function),如果`$\epsilon$`表示的情况出现，则`$I_{\epsilon}=1$`否则`$I_{\epsilon}=0$`
~~~~
这种方法的缺陷是，缓存中一个词的重要性独立于该词与当前词的距离。P. R. Clarkson等人(1997) 的研究表明，缓存中每个词对当前词的影响随着与该词距离的增大呈指数级衰减，因此：
$$
\hat P_{Cache}(w_i|w_1^{i-1})=\beta\sum_{j=1}^{i-1}I_{w_i=w_j}e^{-\alpha(i-j)}
$$
~~~~
$$
\sum_{w_i\in V}P_{Cache}(w_i|w_1^{i-1})=1
$$
其中,`$beta$`衰减率,`$e$`为归一化常数,以使得,`$V$`为词汇表
~~~~
##### (2) 基于混合方法的语言模型

该方法针对的问题是：由于大规模训练语料本身是异源的(heterogenous)，来自不同领域的语料无论在主题(topic)方面，还是在风格(style)方面，或者两者都有一定的差异，而测试语料一般是同源的(homogeneous)，因此，为了获得最佳性能，语言模型必须适应各种不同类型的语料对其性能的影响。

~~~~
处理方法:将语言模型划分成`$N$`个子模型`$M_1,M_2,...,M_n$`整个语言模型的概率通过下面的线性插值公式计算得到。
`$
\hat P(w_i|w_1^{i-1})=\sum_{j=1}^n\lambda_j\hat P_{M_j}(w_i|w_1^{i-1})
$`
其中,`$0\leq\lambda\leq 1,\sum_{j=1}\lambda_j=1$`值可以通过EM算法计算出来
~~~~
1. 对训练语料按来源、主题或类型等聚类；
1. 在模型运行时识别测试语料的主题或主题的集合；
1. 确定适当的训练语料子集，并利用这些语料建立特定的语言模型；
1. 利用针对各个语料子集的特定语言模型和线性插值，获得整个语言模型。

~~~~

##### (3) 基于最大熵的语言模型

基本思想:通过结合不同信息源的信息构建一个语言模型。每个信息源提供一组关于模型参数的约束条件，在所有满足约束的模型中，选择熵最大的模型。

~~~~

### 4.4 神经概率语言模型
~~~~

统计语言模型的目标是学习一种语言中词序列的联合概率函数，但是由于维数灾难的问题让这变得困难。  
n-gram模型是一种近似策略，作了一个马尔可夫假设：认为目标词`$w_i$`的条件概率只与其之前的`$n-1$`个词有关。一个词序列会被测视为与之前训练中出现的所有词序列都不同。通过训练重叠的短的词序列的联系从而得到泛化模型。
~~~~
#### 4.4.1 基本思想
通过学习词的词向量，把每个句子的语义指数告诉模型，从而解决维数灾难的问题。这个模型同时也会学习每个词的词向量和这个词序列的概率函数，并用这种方式表达。如果出现此前从没见过的词序列，并且与出现过的句子在意思上相似有着相似的单词，则它会有一个较高的概率，因此这样就会获得泛化。显著的改善了最先进的n元语法模型，并且这个方法允许使用更长的上下文。
~~~~
#### 4.4.2 相关概念

1. One-Hot Representation（独热）
2. Distributed Representation
~~~~
1. One-Hot Representation（独热）
最早的词向量是很冗长的，大小为整个词汇表的大小，对于每个具体的词汇表中的词，它使用是词向量维度将对应的位置置为1。
比如我们有下面的5个词组成的词汇表，词"Queen"的序号为2， 那么它的词向量就是[0,1,0,0,0]。同样的道理，词"Woman"的词向量就是[0,0,0,1,0]。这种词向量的编码方式我们一般叫做one-hot representation.
~~~~
#### One-Hot Representation存在的问题
最大的问题是我们的词汇表一般都非常大，比如达到百万级别，这样每个词都用百万维的向量来表示简直是内存的灾难。这样的向量其实除了一个位置是1，其余的位置全部都是0，表达的效率不高。
~~~~
2. Distributed Representation
基本思想是通过训练，将每个词都映射到一个较短的词向量上来。所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。这种向量可以表示为：
[0.792, −0.177, −0.107, 0.109, −0.542, …]。
维度以50维和100维比较常见
~~~~
#### 4.4.3 基本结构
神经概率语言模型可以分为四层，即输入层，embedding层，隐藏层，输出层。（也可以分为三层，即把embedding层当做输入层，然后隐藏层、输出层，其实在实际操作中实际的输入就是embedding层（词向量））
~~~~
##### CBOW模型
![image](https://img-blog.csdnimg.cn/20190605104659312.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5MTYxMjE2,size_16,color_FFFFFF,t_70)
~~~~
##### skip-gram模型
![image](https://img-blog.csdnimg.cn/20190605104845595.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5MTYxMjE2,size_16,color_FFFFFF,t_70)
~~~~
1. 输入层  
用词在词表中的位置（下标），比如几个单词：我 是 教师，在词表中的位置是[3,220]，只要存这几个下标就行了。然后根据下标去embedding层的矩阵中找对应的行。
~~~~
2. embedding层
embedding层实际上是一个矩阵，这个矩阵又叫look-up table。这个矩阵长这个样子，`$|V|$`行数为 ，就是词表的大小，列数为`$h_1$`，即每个词向量的维度，这个维度一般要比one-hot表示的词向量（维度为词表大小）小很多。矩阵的值开始时随机初始化。这样矩阵的每一行就为一个词向量（BP(back propagation)后，每行都是`$h_1$`的实数），可以用来表示一个单词。
~~~~
3. 隐藏层
用的sigmoid函数作为激活函数，bengio的论文中用的tanh函数
4. 输出层
输出层用的softmax函数

~~~~
### 4.5 语言模型的评价方法

- 最佳方法:  
 将语言模型用于某一个应用中，例如拼写检查、机器翻译等

~~~~
- 困惑度(Perplexity):   
假定测试语料`$T$`由`$n$`个句子组成`$T={S_1,S_2,...,S_n }$`则整个测试语料的概率为:
$$
P(T)=\prod_{i=1}^nP(S_i)
$$
~~~~
以对数形式表示的测试语料的概率为：
$$
\log P(T)=\log \prod_{i=1}^nP(S_i)=\sum_{i=1}^n\log P(S_i)
$$
~~~~
测试语料的交叉熵为：
$$
H_p(T)=-\frac{1}{W_T}\log_2P(T)=-\frac{1}{W_T}\sum_{i=1}^n\log_2P(S_i)
$$
~~~~
困惑度(Perplexity): 
$$
P(T)=2^{H_p(T)}
$$
~~~~
### 4.6 语言模型的应用
~~~~
#### 汉语分词问题
方法描述,设对于待切分的句子`$S=z_1,z_2,...z_m,W=w_1,w_2,...,w_k(1\leq k\leq n)$`是一种可能的切分。那么
$$
W=\arg \max P(W|S)=
$$
$$
\arg \max P(W)P(S|W)\cong \arg \max P(W)
$$
~~~~

<!--
#### 4.7.2 word2vec
基本思想是通过训练将每个词映射成`$K$`维实数向量(`$K$`一般为模型中的超参数)，通过词之间的距离（比如 cosine相似度、欧氏距离等）来判断它们之间的语义相似度.其采用一个三层的神经网络，输入层-隐层-输出层。有个核心的技术是根据词频用Huffman编码
-->

### 4.7 语言模型的进展
参阅[Two decades of statistical language modeling:  where do we go from here](https://kilthub.figshare.com/articles/Two_Decades_of_Statistical_Language_Modeling_Where_Do_We_Go_From_Here_/6611138/files/12103316.pdf)
~~~~

### 练习
阅读《统计自然语言处理基础》第6章例子，下载该书配套的[Austen 语料库(包括训练集和测试集)](http://nlp.stanford.edu/fsnlp/)。

尝试利用MLE、Good turing估计建立Austen语料库的bigram模型。并将计算结果与参考书给出的部分结果进行比较。

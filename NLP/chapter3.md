## 第三章 自动分词标注

### 主要内容
1. 汉语自动分词方法
2. 词性标注方法
3. 命名实体识别
4. 新词识别
~~~~

汉语自然语言处理的第一项核心技术：中文（或汉语）词汇自动切分，也称为中文分词技术。从1979年，中国就开始进行机器可读语料库的建设，专业的高校和研究机构纷纷建立大规模中文语料库。这个阶段历经十多年之久，由于语料库建设之初，许多工作都要从零开始，分词任务都由专业人员手工完成。这是一项繁重而枯燥的工作。即便如此，受到人为主观因素的影响，人工分词的标准并不统一，语料精度也不高。虽然是国家级的项目，所谓的“大规模”语料库的规模不过也就是百万级。因此，迫切需要统一的分词规范及适合大规模语料的高精度中文分词算法。
~~~~
中文分词的研究经历了二十来年，现在看来基本上分为如下三个流派。
机械式分词法（基于词典）
基于语法和规则的分词法
基于统计的分词法
~~~~
机械式分词法（基于词典）。机械分词的原理是将文档中的字符串与词典中的词条进行逐一匹配，如果词典中找到某个字符串，则匹配成功，可以切分，否则不予切分。基于词典的机械分词法，实现简单、实用性强，但机械分词法的最大的缺点就是词典的完备性不能得到保证。据统计，用一个含有70 000个词的词典去切分含有15 000个词的语料库，仍然有30%以上的词条没有被分出来，也就是说有4 500个词没有在词典中登录。
~~~~
基于语法和规则的分词法。其基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来进行词性标注，以解决分词歧义现象。因为现有的语法知识、句法规则十分笼统、复杂，基于语法和规则的分词法所能达到的精确度还远远不能令人满意。目前这种分词系统还处在试验阶段。
~~~~
基于统计的分词法。其基本原理是根据字符串在语料库中出现的统计频率来决定其是否构成词。词是字的组合，相邻的字同时出现的次数越多，就越有可能构成一个词。因此，字与字相邻共现的频率或概率能够较好地反映它们成为词的可信度。
~~~~
针对这些问题，经过20年来的不懈努力，最终较成功地实现了中文词汇的自动切分技术。本章简要介绍了ICTCLAS中文分词算法的来源和现状，以及实现了NShort最短路径算法的一些著名的开源框架。为了读者理解方便，我们选择HaNLP系统提供的开源框架，结合实例，详细分析和讲解著名的NShort最短路径分词方法。内容包括：一元词网与原子切分、生成二元词图、NShort最短路径、命名实体识别、细分阶段等内容。
~~~~

信息处理的目标是使用机器（主要指计算机）能够理解和产生自然语言。而自然语言理解和产生的前提是对语言能够做出全面的解析。汉语词汇是语言中能够独立运用的最小的语言单位，是语言中的原子结构。因此，对中文进行分词就显得至关重要。
~~~~
## 什么是词与分词规范
1996年，有人通过6个母语为汉语的被试者对同一篇文本进行手工分词，文本由100个句子组成，含4 372个字。被试者共6人，其中三位来自内地，三位来自我国台湾地区。人们因自身居住地区的不同、受教育程度等因素，对词汇的切分产生的差异，称为词语认同率。与人们的猜测有很大的不同，两个地区的人对词汇的认同产生了比较大的差异。其中，差异最大的仅为0.69，最小为0.89，平均认同率为0.76。（《中文分词十年回顾》）由此可见，即便是母语为汉语的使用者，对于划分词汇的标准也是有分歧的。

那么，什么是词，我们如何界定汉语词呢？古往今来，汉字虽然有5万多个，但常用的汉字大约仅有6 000个。即便如此，其中很多汉字在日常生活中较少用到。然而，这些有限的汉字足以维持词汇的长期更新，因为扩大中文词汇的方法是通过构造汉字的复合新词，而不是创造新的字符来完成的。这就造成了汉语中所谓的词和短语之间没有明确的界限。这可能也就是中国的一些语法学家认为，中文没有词语而只有汉字的原因，并创造了一个术语——“字短语”来代替传统的词汇。董振东就认为：“‘词或字符’的争论源于他们都急于给中国语言一个硬规范的共同基础。遗憾的是，中文不是那么明确的或硬的，它是软的。我们必须认识到其‘柔软度’”。

除构词法的原因之外，人们还因为自身的方言、受教育程度、亚文化等差异因素，对词汇的认识也不同。这些事实都成为制定统一的汉语分词标准的障碍。但是对于计算机系统而言，同一种语言应使用统一的规范来划分词汇，否则，很难想象在多重标准下的词汇系统能够计算出相同的语义结果。这是计算机系统所提供的规定性。因此，构建一套统一的分词规范就显得尤为重要。

随着NLP的大规模应用，计算语言学界逐渐统一了汉语词汇的标准。从最初的“结合紧密，使用稳定”到信息处理领域的《信息处理用现代汉语分词规范》的制定，都是确定汉语分词标准的一种尝试，该文关于汉语词的定义给出了如下说明。

汉语信息处理使用的、具有确定的语义或语法功能的基本单位。⋯⋯

——《信息处理用现代汉语分词规范》

从计算语言学的角度来看，如果把一个句子理解为一个特殊的可计算的逻辑表达式，那么句子中的一个词就是表达式中的一个可计算符号，有的表示为连接的符号，如连词“然后”、“而且”这样的虚词；有的表示为动作、状态（函数的签名），如“出现”、“思考”等这样的动词；有的表示为事物的概念，如“中国”、“泰山”等这样的名词。

本书涉及的分词规范有如下两大类：第一类包括《北大（中科院）词性标注》、《现代汉语语料库加工规范——词语切分与词性标注》、《北京大学现代汉语语料库基本加工规范》三篇文章，读者可从http://www.threedweb.cn/thread-1584-1-1.html、http://www. threedweb.cn/thread-437-1-2.html下载；第二类为《宾州树库中文分词规范》，读者可从http://www.threedweb.cn/thread-1478-1-1.html下载。

本节主要介绍中文分词中最常用的《北大（中科院）词性标注》（以下简称《北大规范》）的基本原则。

《信息处理用现代汉语分词规范》和传统的语法教育中将汉语的词类主要分为13种：名词、动词、代词、形容词、数词、量词、副词、介词、连词、助词、语气词、叹词和象声词。这与朱德熙先生提出的19种分类法有所不同。朱先生的分类法还包括：时间词、处所词、方位词、区别词、状态词等。除此之外，《北大规范》还加入了4个兼类谓词：副动词、名动词、副形词、名形词；最后还增加了前缀、后缀、成语、简称、习用语5种辅助词类。这样，《北大规范》就形成了40种词类。

这40种词类与《信息处理用现代汉语分词规范》所述的13种词类可以用表给出对照关系（见表3.1）。
~~~~

## 两种分词标准

由于语素对词汇的构成也产生影响，实际应用中，汉语分词也分为两个粒度。粗粒度分词：将词作为语言处理最小的基本单位进行切分。细粒度分词：不仅对词汇进行切分，也要对词汇内部的语素进行切分。

例如，原始串：浙江大学坐落在西湖旁边。

粗粒度： 浙江大学 /坐落/在/西湖/旁边。

细粒度： 浙江/大学 /坐落/在/西湖/旁边。

粗粒度将“浙江大学”看作一个完整的概念，对应一个完整的词汇，进行切分。而细粒度则不同，除将“浙江大学”完整切分出来之外，还要将构成“浙江大学”的各个语素切分出来：浙江/大学。

常见的例子还有很多，如“中华人民共和国”，粗粒度的分词就是“中华人民共和国”，细粒度的分词可能是“中华/人民/共和国”。一般细粒度切分的对象都为专有名词。因为专有名词常表现为几个一般名词的合成。

在实践中，粗粒度切分和细粒度切分都有其使用的范围。粗粒度切分主要用于自然语言处理的各种应用；而细粒度分词最常用的领域是搜索引擎。一种常用的方案是，在索引的时候使用细粒度的分词以保证召回，在查询的时候使用粗粒度的分词以保证精度。在本书中，如果未加特别的说明，则都为粗粒度分词。
~~~~
## 歧义、机械分词、语言模型

~~~~
## 词汇的构成与未登录词

能够正确地消除切分中的歧义，使计算机处理词汇问题又前进了一大步。但是问题仍旧没有完全解决。所谓词汇，一般都具有三个重要的特性：稳固性、常用性和能产性。稳固性、常用性都比较容易理解。关键问题在于能产性。前面讲过有关汉语复音词的构词方法，与拼音文字不同，汉语的构词机制是一个动态的自组织认知系统，而不是一个静态的系统。随着外界新事物的产生，表达概念的新词汇也会层出不穷地涌现，这不是什么特别的技能，对于中国人而言是一种本能，几乎人人都具备。人们给小孩子起名字就是词汇的能产性生动的体现：一方面，孩子的名字要继承父辈的姓氏，另一方面又要传达大人对孩子未来最美好的期望，而又尽可能不与他人的名字重复。这个过程就是词汇能产性的表现。网络媒体、专业术语、组织机构的命名都是新词产生的重要来源。

在自然语言处理中，它们被统称为未登录词识别（Named Entity Recognition, NER）。在真实文本的切分中，未登录词总数的大约九成是专有名词，其余的为通用新词或专业术语。因此，未登录词识别就是包括中国人名、译名、日本人名、地理位置名称、组织机构名称等专有名词的识别。在自然语言处理研究中，人们通常将上述专有名词和数字、日期等词称为命名实体。由于命名实体识别不仅是汉语自动分词研究中的关键问题，也是诸如英语等其他语言处理中的重要问题，它的处理效果直接影响到信息抽取、信息检索和机器翻译、文摘自动生成等应用系统的性能。因此，近几年来专有名词的处理（包括识别、翻译等）已经成为自然语言处理研究中一个非常活跃的分支。

以往人们的处理方法常从构词学的角度来研究算法，因此研发了很复杂的基于构词编码的方法（下文介绍的HanLP中文分词的系统就是一种基于构词角度来进行命名实体识别的算法）。事实证明，这种做法对于狭窄的专门领域（人名中的中国人名、译名、日本人名）等的未登录词识别，能够获得较好的效果，但在处理大规模不同领域的未登录词问题上存在着很大的障碍，至少是不现实的。

笔者认为应从语义类的角度来重新考虑这个问题。不同语义类下的未登录词，在统计学规律上具有相似性。利用这一点，近些年命名实体识别方面新的算法层出不穷，已经证明，基于半监督的条件随机场（semi-CRF）算法，对于处理不同领域的专名识别具有较低的成本和较好的效果。
~~~~
### 汉语自动分词方法
1. 正向最大匹配
2. N-最短路径方法
3. 基于词的n元语法模型的分词方法
4. 由字构词的汉语分词方法
5. 基于词感知机算法的汉语分词方法
6. 其他分词方法


### 最大匹配

```
# -*- coding:utf-8 -*-
#!/usr/bin/python3


def load_Dictionary():
    dic={}
    path = "/Users/Jackie/Desktop/usr.dic"
    file = open(path,"r",encoding="UTF-8")
    for line in file:
        array = line.strip().split(" ")
        print(array)
        attr ={}
        attr["natrue"]=array[2]
        attr["freq"]= array[1]
        word = array[0]
        dic[word]=attr
    print(dic)
    return dic

#sline是要切分的字符串
def FMM(wordDict, sline):
    maxLen=3
    wordList = []#用作存储已切分的词
    slineLen = len(sline)#待切分字符串的长度
    while slineLen > 0:#待切分字符串长度大于0
        if (slineLen > maxLen):
            wordLen = maxLen
        else:
            wordLen = slineLen
        subStr = sline[0:wordLen]
        while wordLen > 1:#截取的字符串长度大于1
            if (subStr in wordDict):
                break
            else:
                wordLen = wordLen - 1
                subStr = subStr[0:wordLen]
        wordList.append(subStr)
        print(subStr)
        sline = sline[wordLen:]
        slineLen = slineLen - wordLen

    print(wordList)
    return wordList


def RMM(wordDict, sline):
    maxLen=3
    wordList = []#用作存储已切分的词
    slineLen = len(sline)#待切分字符串的长度
    while slineLen > 0:#待切分字符串长度大于0
        if (slineLen > maxLen):
            wordLen = maxLen
        else:
            wordLen = slineLen
        subStr = sline[slineLen-wordLen:]
        while wordLen > 1:#截取的字符串长度大于1
            if (subStr in wordDict):
                break
            else:
                wordLen = wordLen - 1
                subStr = subStr[1:]
        wordList.append(subStr)
        print(subStr)
        sline = sline[0:slineLen-wordLen]
        slineLen = slineLen - wordLen
    wordList.reverse()
    # print(wordList)
    return wordList

def tag(wordDict, wordList):
    wlist =[]
    for word in wordList:
        if( word in wordDict):
            value = wordDict[word]
            natrue = value["natrue"]
            wlist.append("%s/%s"%(word,natrue))
        else:
            natrue= "x"
            wlist.append("%s/%s"%(word,natrue))
    return wlist

#入口函数
if __name__ == '__main__':
    #HelloWorld()
    usrdic= load_Dictionary()
    # print(usrdic)
    line = "今天天气不错"
    # FMM(usrdic,line)
    wordList =RMM(usrdic,line)

    wordList = tag(usrdic,wordList)
    resultLine = " ".join(wordList)
    print(resultLine)
```

### jieba分词系统

### pynlpir分词系统

### 其他中文分词系统
SnowNLP
THULAC
pkuseg
pyhanlp
FoolNLTK
pyltp
Stanford CoreNLP
### 其他语言分词系统

1. MeCab
2. paoding

### 2. 词性标注方法
1. 基于规则的词性标注方法
2. 基于统计模型的词性标注方法
3. 基于统计方法与规则方法相结合的词性标注方法
4. 基于深度学习的词性标注方法

### 3. 命名实体识别

### 4. 新词识别
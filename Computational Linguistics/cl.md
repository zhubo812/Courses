# 计算语言学基础理论

<br>
渤海大学文学院

朱波

zhubo812@gmail.com

----
### 成绩评定
1. 平时成绩：作业+出勤（30%）
2. 期末作业（70%）
----

### 学习目标

1. 具备<font color = yellow>以计算机为背景思考语言学问题</font>，对<u>语言知识</u>进行<font color = yellow>形式化描述</font>的能力；
2. 了解计算语言学的基本理论和自然语言处理中的常用模型及算法，初步具备从事相关领域研究工作的能力。

----

### 主要参考书目

1. Introduction to Natural Language Processing, Harris,M.D.,Reston Publishing Co.,1985

2. Speech and Language Processing, Jurafsky, D.&Martin,J.H., PrenticeHall,2000(中译本：自然语言处理综论，冯志伟等译，电子工业出版社，2005)

3. Foundations of Statistical Natural Language Processing, Manning, C.D.&Schütze, The MIT press, 1999(有中译本)

4. Natural Language Understanding,Allen,J.,TheBenjamins/CumminsPublishingCo.,1994(有中译本)

----

   

5. Natural Language Processing: An Introduction to Computational Linguistics, Gazdar, G.&Mellish, C.,AddisonWesley,1989.

6. 计算语言学概论，俞士汶主编，商务印书馆，2003.

7. 现代汉语语法信息词典详解，俞士汶等，清华大学出版社，2003

8. 自然语言理解，姚天顺，清华大学出版社，2002

9. 自然语言处理技术基础，王晓捷、常宝宝，北京邮电大学出版社，2002
----

10. 计算语言学，刘颖，清华大学出版社，2002

11. 计算语言学基础，冯志伟，商务印书馆，2001

12. 计算语言学导论，翁富良、王野翊，中国社会科学出版社，1998

13. 自然语言的计算机处理，冯志伟，上海外语教育出版社，1997

14. 自然语言处理，刘开瑛、郭炳炎，科学出版社，1991

----
### 相关学术期刊
1. Computational Linguistics
2. Machine Translation
3. International Journal of Corpus Linguistics
4. 中文信息学报(中文信息学会)
5. 计算机学报
6. 软件学报
7. 汉语语言与计算学报
----
###  相关学术会议
1. [Annual Meeting of the Association for Computational Linguistics(ACL)](https://www.aclweb.org/anthology/)
2. International Conferenceon Computational Linguistics(COLING)
3. 全国计算语言学联合学术会议(JSCL)[CCL历年论文集,NLP-NABD历年论文集](http://cips-cl.org/anthology)
4. 全国学生计算语言学研讨会(SWCL)

---

### 课程内容
1. 概述
2. 相关概念及基础知识
3. 形式语言与自动机
4. 语言模型
5. 自动分词与词性标注
6. 句法分析
7. 语法理论
8. 语义计算
9. 机器翻译


---




## 概述

1. 计算语言学的性质及特点
2. 计算语言学的发展及学科位置

----
### 计算语言学的性质及特点
1. 计算语言学的<font color = yellow>定义</font>
2. 计算语言学的<font color = yellow>特点</font>
3. 计算语言学的<font color = yellow>研究方法</font>
4. 计算语言学对人们<font color = yellow>语言观的影响</font>

----
#### 定义


计算语言学的中心任务是开发研制出一种人类能用自己的语言与之自由交谈的智能计算机。

——(德国计算语言学家 Roland Hausser,*Foundations of Computational Linguistics*）

----
Question

计算机能理解、会思维吗？
----
英国天才的数学家、计算机科学家图灵（Turing）提出了“图灵测试”,即,如果有<font color = yellow>超过30%的测试者不能确定出被测试者是人还是机器</font>，那么这台机器就通过了测试，并被认为具有人类智能。

![timg](pic\timg.jpg)

----
根据“图灵测试”的思想，计算机如果做到了下面四条中的一条，就可以认为它是智能的：
1. 问答（question-answering）
2. 生成文摘（summarizing）
3. 释义（paraphrase）
4. 翻译（translation）

----
#### 定义

计算语言学是通过建立形式化的计算模型，用计算机分析处理、理解并生成自然语言的学科。
----
自然语言生成（Nature Language Generation,NLG）

自然语言生成从知识库或逻辑形式等等机器表述系统去生成自然语言,可以说是一种将资料转换成自然语言表述的翻译器。
----
NLG一般通过多个子任务来解决问题。 一般可以分为以下六类：
1. 内容确定（Content determination）：决定在建文本中包含哪些信息；

2. 文本结构（Text structuring）：确定将在文本中显示的信息；

3. 句子聚合（Sentence aggregation）：决定在单个句子中呈现哪些信息；
----
4. 词汇化（Lexicalisation）：找到正确的单词和短语来表达信息；

5. 引用表达式生成（Referring expression generation）：选择单词和短语以识别域对象；

6. 语言实现（Linguistic realisation）：将所有单词和短语组合成格式良好的句子。
----
对计算语言学，也有广义和狭义两种理解。上述是狭义的理解。

广义的计算语言学不仅包括上述内容，还包括利用计算机研究自然语言的有关问题，如风格统计、词语计量研究等等。

----
| 作者   | 句平均词数 | 句平均字数 | 平均词长 |
| ------ | ---------- | ---------- | -------- |
| 巴金   | 24.75      | 40.65      | 1.642    |
| 倪海曙 | 15.79      | 24.05      | 1.523    |

----
《静静的顿河》的作者是…肖洛霍夫？克留柯夫？

1. 平均句长
2. 句长分档，每档百分比
3. 不同词类统计
4. 不同词类在句中的顺序
5. 某些词汇特点的统计
6. 比较样品中使用一次、二次、三次……的词
----
《红楼梦》的作者是一个人？还是两个人？

1. 上下文相关性（型例比/种次比）
2. 不同字符数的统计
3. 字符串的统计

----

##### 统计结果：
1. 前八十回<后四十回
2. 前八十回>后四十回
3. 前八十回双音节字符串<单音节字符串，后四十回双音节字符串>单音节字符串

----
##### 结论
作者应该是两个人

----
#### 语言计量研究的基本概念

- 词种（type）
- 词次（token）

----
句子“我明天要去上海看世博会，你去不去？” 的<font color = yellow>词种数</font>和<font color = yellow>词次数</font>分别是多少？
----
- 词种：9
- 词次：11
----
字符种？字符次？

----
#### 语言计量研究涉及的问题

1. 计量研究的目标是什么？
目标决定对象、方法和过程。

2. 如何得到数据？
需要掌握各种统计工具、计算方法。

3. 如何对数据进行分析？
综合运用各种知识挖掘数据背后潜藏的东西。


----
#### 计算语言学的特点

----
计算语言学的特点1

**元语言的形式化**
----
对象语言与元语言：

- 对象语言，就是你所要研究的那种语言;
- 元语言，是指研究者在研究描述对象（语言）时使用的语言。

----
**元语言的形式化**
1. 用自然语言作元语言的缺陷是歧义太多。
2. 数学公式、逻辑符号等都是形式化的元语言。
3. 形式化的元语言是人和计算机沟通的必要途径。
4. 计算语言学研究的是元语言应该如何设计才能更好地描述对象语言里的规律，而且计算机能够读懂。

----
**规则描述语言**

1. 一|这|那/Q1/ (a)/ n* = N4 [NP>N1 N2 of N3 N4]
2. a)/ n|r* = N2 [NP > N1 N2]
3. d/ a* = N2 [a > N1 N2]



----
计算语言学的特点2

**可操作性**

----
<font color = yellow>可操作性</font>是计算语言学最根本、最关键的方法论原则就是要指出各种语言形式出现和变换的条件。
----
所谓的<font color = yellow>条件</font>和与之想关联的<font color = yellow>动作</font>是一切计算机工作的最基本的方式，也是建立计算语言学语法的最基本、最关键的公式。
----
计算语言学的研究手段是计算，计算的<font color = yellow>表现形式</font>是算法。

算法是对解题方法的精确描述,可表示为一组有穷的操作规则。
1. 通用性：算法是针对一类问题的
2. 机械性：算法的每一步骤都是确定的
3. 有限性：算法必须在有限步骤内结束
4. 离散性：算法的输入及输出数据都是离散符号
----
N+N短语

- 偏正关系：学校图书馆、木头桌子
- 并列关系：工人农民、爸爸妈妈
- 复指关系：首都北京、鲁迅先生
- 主谓关系：今天星期二、鲁迅浙江人
----
今天星期六、今天三八节（主谓）

今天下午，今年三八节（偏正）？
----

计算语言学的特点3

**工程性**
----
1. 机器翻译（Machine Translation）
2. 文本分类（Text Classification）
3. 自动文摘（Automatic Text Abstraction）
4. 信息检索（Information Retrieval）
5. 信息提取（Information Extraction）
6. 语音合成（Speech Synthesis）
7. 语音识别（Speech Recognition）
8. ……
----
从本质上说，计算语言学是一门实验性、工程性的学科。
----
计算语言学的特点4

**语言研究的全局性、一般性**
----
歧义现象

- 咬死了猎人的狗（动宾/偏正，实例歧义）
- 咬死了猎人的鸡（动宾）
- 咬死了鸡的老虎（偏正）

VP + NP1 + 的 + NP2（格式歧义、潜在歧义）
----
#### 计算语言学的特点
1. 元语言的形式化
2. 可操作性
3. 工程性
4. 语言研究的全局性、一般性

----
#### 计算语言学的研究方法
----
1. 规则驱动的方法
2. 数据驱动的方法
3. 二者融合的方法

----
规则驱动的方法
1. 研究人员（例如语言学家）对语言的规律进行总结，形成规则形式的知识库。
2. 研制语言处理算法，利用这些规则对自然语言进行处理。
3. 研究人员根据处理结果，调整规则，改进处理效果。
----

根据如下6条规则，分析句子“The boy saw the girl with a telescope.”，并画出树形图。


1. `$S\rightarrow NP+VP$`
2. `$NP\rightarrow DET+N$`
3. `$NP\rightarrow NP+PP$`
4. `$VP\rightarrow VP+PP$`
5. `$VP\rightarrow V+NP$`
6. `$PP\rightarrow P+NP$`

----
<font color = yellow>规则驱动的方法的问题</font>

All grammar leak(Sapir1921)认为：

对于自然语言而言，不大可能写出一部完备的规则集，语言规则有很强的伸缩性。
一般而言，很多基于规则的系统不能满足真实语言文本处理的要求，而只能处理真实语言的某个很小的子集。
----
数据驱动的方法（统计方法）
----

All grammar leak(Sapir1921)

1. 建立可以反映语言使用情况的语料库；
2. 研究人员对自然语言进行统计；
3. 建模利用统计技术或机器学习技术，基于语料库训练统计语言模型；
4. 利用得到的模型设计算法对语言进行处理；
5. 根据处理效果改进模型，提高处理性能。

----

在数据驱动的方法中，语言模型通常体现为一组参数，这些参数通常表示某个语言形式发生的概率值。

对一个由三个词构成的字符串来说，其发生的概率可以表示为：`$P(w_3|w_1,w_2)$`

----
判断“今天是周末”和“今天是课堂”哪个句子概率大?

𝑃(周末│今天是)>𝑃(课堂│今天是)

----
<font color = yellow>数据驱动的方法（统计方法）的问题</font>

数据驱动的方法忽视了语言的深层结构。

----
二者融合的方法

1. 融合规则驱动、数据驱动的优劣不能简单评价
2. 很多研究人员（包括知名计算语言学家）建议如此
3. 已经提出了一些策略，但如何结合尚须进一步探索

----
两种方法的区别
----
1. 对研究对象语言知识的认识不同。

- 规则方法：语言知识在人的大脑里，即人的语言能力；(<font color = yellow>语言</font>)
- 统计方法：语言知识在语言数据之中。(<font color = yellow>言语</font>)

----
2. 获取语言知识的方法和途径不同。
- 规则方法：用内省方法，建立形式化的知识系统描述语言知识；
- 统计方法：用语料库方法，根据对语言数据的统计得到语言知识。
----
3. 使用语言知识K构建语言处理系统时使用的算法不同。

- 规则方法：发展出许多比较成熟的算法技术；
- 统计方法：主要使用概率统计模型的自然语言处理算法。
----
4. 对语言事实的评价不同。

- 规则方法：基于乔姆斯基的语言原则，语句是<font color = yellow>正确的/错误的</font>；
- 统计方法：基于先农(Shannon)的信息论,语句是<font color = yellow>常见的/罕见的</font>。


----
#### 计算语言学对人们语言观的影响
----
传统的语言观

- 语言是人类最重要的交际工具；
- 语言是一个符号系统。
----
 新的语言观

- 语言是人类最重要的交际工具；
- 语言是人类和计算机共同的表达知识、传递信息、实施控制的符号系统。
----
新的语言观

1. 语言是人类和计算机的共同的交际工具；
2. 语言是表达知识的最有效方法；
3. 语言是传递信息的最主要的载体；
4. 语言是实施控制的最方便的手段；
5. 语言是一个多层面的符号系统。
	- 语构层面:符号与符号之间的形式结构关系---句法学；
	- 语义层面:符号与其所指之间的意义关系---语义学；
	- 语用层面:符号与使用者之间的效用关系---语用学。
----
### 计算语言学的发展和学科位置
----
#### 计算语言学的发展

计算语言学的历史可以大致分为萌芽期、发展期和繁荣期三个阶段。

----
**1. 萌芽期**(1956年以前)

- <圣经•创世纪>巴别塔
- 20世纪30年代初，法国科学家阿尔楚尼（Artsouni）的“机械脑”（mechanicalbrain）;
- 1933年，苏联发明家特洛扬斯基（Tроянский）设计了语言翻译机.
- 1946年，美国宾夕法尼亚大学的埃克特（J.P.Eckert）和莫希来(J.W.Mauchly)设计并研制出世界上第一台电子计算机爱尼亚克(ENIAC)。
- 同年，美国洛克菲勒基金会副总裁韦弗(W.Weaver)和英国工程师布斯(A.D.Booth)在讨论电子计算机的应用范围时，就谈到了利用计算机进行语言自动翻译的想法。
- 1949年，韦弗发表题为《翻译》的备忘录，正式提出机器翻译问题。
- 1954年，美国乔治敦大学&IBM公司，用IBM-701计算机，进行了世界第一次机器翻译试验，把几个简单的俄语句子翻译成英语。

----
**机器翻译(Machine Translation,简称MT）**

机器翻译是利用计算机把一种语言的语句自动地转换为语义与之完全相同的另一种语言的语句。

----
| 源文   | 正确的译文 | 机器的译文 | 
| ------ | ---------- | ---------- | 
| How are you?   | 你好吗？      | 怎么是你？      |
| How old are you? | 你多大了？     | 怎么老是你      | 

----

**2. 发展期(1957-20世纪70年代初)**


早期：<font color = yellow>关键词</font>匹配为主流

- [1968年维金鲍姆（J·Weizenbaum）的ELIZA系统 （模拟心理医生、能与患者谈话的系统）](http://www.manifestation.com/neurotoys/eliza.php3)
----
中后期：以<font color = yellow>句法-语义分析</font>技术为主流

- 维诺格拉德于1972年在美国麻省理工学院建成著名的SHRDLU系统（积木世界）。 
----
![SHRDLU系统（积木世界）](
pic/SHRDLU.jpg)

下面是 SHRDLU系统中人和计算机之间的一段对话：
- <font color = yellow>Person</font>: Pick up a big red block.
	- <font color = red>Computer</font>: Ok. (does it)
- <font color = yellow>Person</font>: Grasp the pyramid.
	- <font color = red>Computer</font>: I don't understand which pyramid you mean.
- <font color = yellow>Person</font>: Find a block which is taller than the one you are holding and put it into the box.
	- <font color = red>Computer</font>: By "it", I assume you mean the block which is taller than the one I am holding.
----
商克(Schannk)及其学生于1973年在斯坦福大学 建成MARGIE系统。 

MARGIE系统能以推理或释义两种方式运行。在推理方式下，它接受一个输入句，然后利用 推理器生成这个句子所蕴涵的事实。
----

输入 <font color = yellow>John gave Mary some aspirin. </font>（约翰给玛丽一些阿司匹林。）


系统就可以作出推理： <font color = yellow>John believes Mary wants an aspirin. Mary is sick.</font> （约翰相信玛丽需要阿司匹林。玛丽病了。）

----
**在释义方式下，它将以尽可能多的等价方式来复 述输入的句子。**

- 如输入<font color = yellow>John killed Mary by chokingher.</font>(约翰扼杀了玛丽。)

- 系统就会复述出如下的句子：
	- <font color = yellow>John strangled Mary.</font>(约翰勒死了玛丽。) 
	- <font color = yellow>John choked Mary and she died because she was unable to breathe. </font>(约翰扼住玛丽的脖子，她因为不能呼吸就死了。）

----
李家治、陈永明等人建成的“古汉语机器理解系统——ACLUS”。

ACLUS能分析句法，能把《郑人买履》、《刻舟求剑》文言译成现代汉语，并回答故事中的语法和词义问题。
----

- <font color = yellow>问</font>：‘先度其足’中的‘度’是什么意思？ 
	- <font color = red>答</font>：‘度’是动词，是测量某种东西尺寸的意思。 
- <font color = yellow>问</font>：‘吾忘持度’中的‘度’是什么意思？ 
	- <font color = red>答</font>：‘度’是名词，是尺寸的意思。

----
范继淹、徐志敏等人建成的“TK——84汉语人机 对话实验系统”。这是一个铁路客运咨询系统。

- <font color = yellow>问</font>：从北京到大连有高铁吗？
	- <font color = red>答</font>：有。 
- <font color = yellow>问</font>：开往沈阳的快车都有卧铺吗？ 
	- <font color = red>答</font>：11次没有卧铺，其他都有。 
- <font color = yellow>问</font>：11次为什么没有卧铺？ 
	- <font color = red>答</font>：11次8点从北京开，18点49分到沈阳，卧铺没有必要。
----
**3. 繁荣期**(20世纪70年代中后期-20世纪80年代末)

 繁荣期主要以<font color = yellow>机器翻译</font>的出现及发展为标志
----

- 1976年，加拿大蒙特利尔大学与加拿大联邦政府翻译 局联合开发的实用性英法机器翻译系统TAUM-METEO正式投入使用，提供天气预报的翻译服务。
- 美国的SYSTRAN系统
- 日本富士通公司的ATLAS系统
- 德国西门子公司与美国德克萨斯大学联合开发的 METAL系统
----
- 1988年由中国军事科学院开发、中国计算机软件与技术公司投入 市场的第一个机器翻译系统“译星1号”实现了商品化
- 中国社科院与北京高立公司联合开发的“高立英汉机器翻译系统”
- 中科院计算所开发的“863-IMT/EC智能型英汉机器翻译系统”
- 先是国防科技大学开发、后由深圳桑夏公司进一步开发的“桑夏 译王英汉机器翻译系统”
----
**发展期理论建设方面的成果**

- 英国数学家图灵(A·M·Turing)与美国语言学家乔姆斯基 (N·Chomsky)的贡献

----
**发展期句法分析方面的成果**

- 乔姆斯基的<font color = yellow>短语结构语法</font>（phrase structure grammar，简称PSG）
- 伍兹的<font color = yellow>扩充转移网络</font>(augemented transition network,简称ATN)； 
- 卡普兰等的<font color = yellow>词汇功能语法</font>（Lexical-functional grammar，简称 LFG）； 马丁·凯依的<font color = yellow>功能合一语法</font>(functional unificational grammar，简称 FUG)；
- 盖兹达等的<font color = yellow>广义短语结构语法</font>(generalized phrase structure grammar,简称GPSG)等等。

----
**发展期语义分析方面的成果**

- 菲尔摩的<font color = yellow>格语法</font>（case grammar); 
- 威尔克斯的<font color = yellow>优选语义学</font>(preference semantics)
- 商克的<font color = yellow>概念依存理论</font>(Conception Dependency Theory, 简称CD理论)  
----

这一时期计算语言学逐渐融入了人工智能的研究领域。研究方法上出现了基于规则和基于概率两种不同的方法，使得这一时期分为了两大阵营。一个是基于规则方法的符号派(symbolic)，另一个是采用概率方法的随机派(stochastic)。
----
这一时期，两种方法的研究都取得了长足的发展。

- 以Chomsky为代表的符号派学者开始了形式语言理论和生成句法的研究，20世纪60年代末又进行了形式逻辑系统的研究。

- 随机派学者采用基于贝叶斯方法的统计学研究方法。但由于在人工智能领域中，这一时期多数学者注重研究推理和逻辑问题，只有少数学者在研究基于概率的统计方法和神经网络。因此，这一时期基于规则方法的研究明显多于基于概率方法的研究。
----
随着研究的深入，人们发现计算语言学的很多应用在短时间内无法解决，而的新问题新需求又不断地涌现。因此，计算语言学研究受到较大影响。从70年代开始，计算语言学的很多研究领域进入了低谷时期。
但尽管如此，计算语言学仍然在这期间取得了一些成果。例如，
- <font color = yellow>基于隐马尔可夫模型</font>(Hidden Markov Model, HMM);
- <font color = yellow>话语分析</font>(Discourse Analysis)。

之后，由于计算语言学研究者对于过去的研究进行了总结，有限状态模型和经验主义研究方法也开始复苏。

----

**4. 融合期发展期**(20世纪90年代至今）

20世纪90年代中期以后，计算机的处理速度和存储量大幅增加，为计算语言学改善了物质基础，使得语言处理的商品化开发成为可能；另一方面，1994年Internet商业化和同期网络技术的发展使得基于自然语言的信息检索和信息抽取的需求变得尤为重要。

----
**Milestone**
- 2001年 神经语言模型
- 2008年 多任务学习
- 2013年 Word嵌入
- 2013年 NLP的神经网络
- 2014年 序列到序列模型
- 2015年 注意力机制
- 2015年 基于记忆的神经网络
- 2018年 预训练语言模型
----

神经语言模型（Neural language models）

语言模型解决的是在给定已出现词语的文本中，预测下一个单词的任务。这可以算是最简单的语言处理任务，但却有许多具体的实际应用，例如智能键盘、电子邮件回复建议等。当然，语言模型的历史由来已久。经典的方法基于 n-grams 模型（利用前面 n 个词语预测下一个单词），并利用平滑操作处理不可见的 n-grams。
第一个神经语言模型，前馈神经网络（feed-forward neuralnetwork），是 Bengio 等人于 2001 年提出的。

----
![nlm](pic\nlm.png)
----
以某词语之前出现的n个词语作为输入向量。今天，这样的向量被称为大家熟知的词嵌入（word embeddings）。这些单词嵌入被连接并馈入隐藏层，然后将其输出提供给softmax层。
最近，前馈神经网络已经被用于语言建模的递归神经网络和长期短期记忆网络所取代。近年来已经提出了许多扩展经典LSTM的新语言模型。尽管有这些发展，但经典的LSTM仍然是一个强大的基础模型。更好地理解语言模型究竟捕捉了哪些信息，也是当今一个活跃的研究领域。

----
语言建模是无监督学习的一种形式，Yann LeCun也将预测性学习称为获取常识的先决条件。关于语言建模最值得注意的方面可能是，尽管它很简单，但它是本文讨论的许多后期进展的核心：

- Word嵌入：word2vec的目标是简化语言建模。
- 序列到序列模型：这种模型通过一次预测一个单词来生成输出序列。
- 预训练语言模型：这些方法使用语言模型中的表示来进行转移学习。

这意味着NLP的许多重要最新进展可以归结为一种语言建模形式。为了做“真正的”自然语言理解，需要新的方法和模型。

----
多任务学习（Multi-task learning）

多任务学习是在多个任务下训练的模型之间共享参数的一般方法。在神经网络中，这可以通过绑定不同层的权重来轻松完成。多任务学习的想法于1993年由Rich Caruana首次提出，并应用于道路跟踪和肺炎预测（Caruana，1998）。直观地说，多任务学习鼓励模型学习对许多任务有效的表征描述。这对于学习一般的低级表示，集中模型的注意力或在有限量的训练数据的设置中特别有用。
Collobert在2008年首次将多任务学习应用于NLP的神经网络。在这一框架下，词嵌入矩阵被两个在不同任务下训练的模型共享。

----
![mtl](pic\mtl.png)

----
共享单词嵌入使模型能够在单词嵌入矩阵中协作和共享一般的低级信息，这通常构成模型中最大数量的参数。Collobert和Weston在2008年的论文中证明了它在多任务学习中的应用。它引领了诸如预训练单词嵌入和使用卷积神经网络（CNN）之类的方法，这些方法仅在过去几年中被广泛采用。他们也因此获得了2018年机器学习国际会议（ICML）的“时间测试”奖。
多任务学习现在用于各种NLP任务，并且利用现有或“人工”任务已成为NLP指令集中的有用工具。虽然通常预先定义参数的共享，但是在优化过程期间也可以学习不同的共享模式。随着模型越来越多地评估多项任务以评估其泛化能力，多任务学习越来越重要，最近又有提出了多任务学习的专用基准。

----
词嵌入（Word embeddings）

词嵌入在2001年首次出现。而Mikolov等人在2013年作出的主要创新——是通过删除隐藏层和近似目标来使这些单词嵌入的训练更有效。虽然这些变化本质上很简单，但它们与高效的word2vec（word to vector，用来产生词向量的相关模型）组合在一起，使得大规模的词嵌入模型训练成为可能。

Word2vec有两种风格，可以在下面图中看到：CBOW（continuous bag-of-words）和skip-gram。它们的目标不同：一个基于周围的单词预测中心词，而另一个则相反。
----
![wem](pic\wem.jpg)

----
虽然捕获的关系word2vec具有直观且几乎神奇的质量，但后来的研究表明word2vec没有任何固有的特殊性：通过矩阵分解也可以学习单词嵌入和通过适当的调整，像SVD和LSA这样的经典矩阵分解方法可以获得类似的结果。

从那时起，许多工作已经开始探索单词嵌入的不同方面（正如原始论文的引用次数所示）。尽管有许多发展，但word2ve仍然是一种流行的选择并且在今天被广泛使用。Word2vec的范围甚至超出了单词级别：带有负抽样的skip-gram，一个基于本地环境学习嵌入的方便目标，已被应用于学习句子的表示，甚至超越NLP到网络和生物序列等。

一个特别令人兴奋的方向是将不同语言的单词嵌入投影到同一空间中以实现（零射击）跨语言转移。越来越有可能以完全无监督的方式（至少对于类似语言）学习良好的投影，这开启了低资源语言和无监督机器翻译的应用。

----
用于自然语言处理的神经网络（Neural networks for NLP）

2013年和2014年标志着神经网络模型开始在NLP中被采用的时间。三种主要类型的神经网络成为使用最广泛的：循环神经网络（recurrent neural networks）、卷积神经网络（convolutionalneural networks）和结构递归神经网络（recursive neural networks）。
递归神经网络（RNN）是处理NLP中普遍存在的动态输入序列的理想选择。Vanilla RNNs很快被经典的长期短期记忆网络（LSTM）所取代，后者证明其对消失和爆炸梯度问题更具弹性。在2013年之前，仍然认为RNN很难训练；Ilya Sutskever的博士论文是改变这一局面的一个关键例子。LSTM细胞的可视化可以在下图中看到。双向LSTM通常用于处理左右上下文。

----
![nnn](pic\nnn.png)

----
随着卷积神经网络（CNN）被广泛用于计算机视觉，它们也开始应用于语言（Kalchbrenner等，2014；Kim等，2014）。用于文本的卷积神经网络仅在两个维度上操作，其中滤波器仅需要沿时间维度移动。下图显示了NLP中使用的典型CNN。

----
![cnn](pic\cnn.png)
----

卷积神经网络的一个优点是它们比RNN更容易并行化，因为每个时间步的状态仅取决于本地环境（通过卷积运算）而不是像RNN中的所有过去状态。 CNN可以使用扩张的卷积扩展到更宽的感受域，以捕捉更广泛的背景（Kalchbrenner等，2016）。CNN和LSTM也可以组合和堆叠，并且可以使用卷积来加速LSTM。
RNN和CNN都将语言视为一个序列。然而，从语言学的角度来看，语言本质上是等级的：单词被组成高阶短语和子句，它们本身可以根据一组生产规则递归地组合。将句子视为树而不是序列的语言启发思想产生了递归神经网络。

----
![rnn](pic\rnn.png)
----
序列到序列模型（Sequence-to-sequence models）

2014年，Sutskever等人提出了序列到序列学习，一种使用神经网络将一个序列映射到另一个序列的通用框架。在该框架中，编码器神经网络逐符号地处理句子并将其压缩成矢量表示；然后，解码器神经网络基于编码器状态逐个预测输出符号，在每个步骤中将先前预测的符号作为预测下一个的输入.

----
![stsm](pic\stsm.png)
----
机器翻译成了这个框架的杀手级应用。 2016年，谷歌宣布开始用神经MT模型替换其基于单片短语的MT模型（Wu等，2016）。根据Jeff Dean的说法，这意味着用500行神经网络模型替换500,000行基于短语的机器翻译代码。

由于其灵活性，该框架现在是自然语言生成任务的首选框架，不同的模型承担编码器和解码器的角色。重要的是，解码器模型不仅可以以序列为条件，而且可以以任意表示为条件。这使得例如基于图片生成描述（Vinyals等人，2015），基于表格的文本（Lebret等人，2016），基于源的描述、代码更改（Loyola等，2017），以及许多其他应用程序成为可能。

